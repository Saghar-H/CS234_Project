@article{DBLP:journals/corr/BrockmanCPSSTZ16,
  author    = {Greg Brockman and
               Vicki Cheung and
               Ludwig Pettersson and
               Jonas Schneider and
               John Schulman and
               Jie Tang and
               Wojciech Zaremba},
  title     = {OpenAI Gym},
  journal   = {CoRR},
  volume    = {abs/1606.01540},
  year      = {2016},
  url       = {http://arxiv.org/abs/1606.01540},
  timestamp = {Wed, 07 Jun 2017 14:41:24 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BrockmanCPSSTZ16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}
@book{Sutton1998,
abstract = {Presents the book "Reinforcement Learning: An Introduction," written by Richard S. Sutton and Andrew G. Barto and published by the Massachusetts Institute of Technology (MIT) Press in 1998. The book is a textbook targeted toward engineers and scientists in artificial intelligence, operations research, neural networks, and control systems. Examines a computation approach to learning from interaction with environment. 1. Introduction -- 2. Evaluative feedback -- 3. The reinforcement learning problem -- 4. Dynamic programming -- 5. Monte carlo methods -- 6. Temporal-difference learning -- 7. Eligibility traces -- 8. Generalization and function approximation -- 9. Planning and learning -- 10. Dimensions of reinforcement learning -- 11. Case studies.},
author = {Sutton, Richard S. and Barto, Andrew G.},
isbn = {9780262193986},
pages = {322},
publisher = {MIT Press},
title = {{Reinforcement learning : an introduction}},
url = {https://mitpress.mit.edu/books/reinforcement-learning},
year = {1998}
}
@book{Puterman2005,
abstract = {Originally published: New York, N.Y. : John Wiley {\&} Sons, 1994. 1. Introduction -- 2. Model formulation -- 3. Examples -- 4. Finite-horizon Markov decision processes -- 5. Infinite-horizon models : foundations -- 6. Discounted Markov decision problems -- 7. The expected total-reward criterion -- 8. Average reward and related criteria -- 9. The average reward criterion-multichain and communicating models -- 10. Sensitive discount optimality -- 11. Continuous-time models -- App. A. Markov chains -- App. B. Semicontinuous functions -- App. C. Normed linear spaces -- App. D. Linear programming.},
author = {Puterman, Martin L.},
isbn = {0471727822},
pages = {649},
publisher = {Wiley-Interscience},
title = {{Markov decision processes : discrete stochastic dynamic programming}},
year = {2005}
}
@ARTICLE{graphviz,
    author = {Emden R. Gansner and Stephen C. North},
    title = {An open graph visualization system and its applications to software engineering},
    journal = {SOFTWARE - PRACTICE AND EXPERIENCE},
    year = {2000},
    volume = {30},
    number = {11},
    pages = {1203--1233}
}
@inproceedings{networkx,
author = {Aric A. Hagberg and Daniel A. Schult and Pieter J. Swart},
title = {Exploring network structure, dynamics, and function using {NetworkX}},
year = {2008},
month = Aug,
urlpdf = {http://math.lanl.gov/~hagberg/Papers/hagberg-2008-exploring.pdf},
booktitle = {Proceedings of the 7th Python in Science Conference (SciPy2008)},
editors = {G\"{a}el Varoquaux, Travis Vaught, and Jarrod Millman},
address = {Pasadena, CA USA},
pages = {11--15}
}
@techreport{ebnf,
  title={Extended BNF-a generic base standard},
  author={Scowen, Roger S},
  year={1998},
  institution={Technical report, ISO/IEC 14977. http://www.cl.cam.ac.uk/mgk25/iso-14977.pdf}
}